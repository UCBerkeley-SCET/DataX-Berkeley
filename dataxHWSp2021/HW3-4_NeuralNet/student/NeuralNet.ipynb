{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter Grader\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XAmi_zzB9U41"
   },
   "source": [
    "![data-x](https://raw.githubusercontent.com/afo/data-x-plaksha/master/imgsource/dx_logo.png)\n",
    "\n",
    "___\n",
    "\n",
    "#### NAME:\n",
    "\n",
    "#### STUDENT ID:\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vLbcKWIl4_d"
   },
   "source": [
    "# **HW3-4: Neural Network**\n",
    "**(Total 120 points)**\n",
    "\n",
    "Before we get started, we'd like to introduce you to a [\"neural network playground\"](https://playground.tensorflow.org) that lets you visualize and tinker with neural networks. It is especially helpful if Artificial Neural Network is a new thing to you.\n",
    "\n",
    "The notebook utilizes [tensorflow](https://www.tensorflow.org/overview) to create and train neural networks. More specifically, we will use the high-level `tensorflow.keras` [API](https://www.tensorflow.org/api_docs/python/tf/keras). Run the code cell to load necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lmcKaRn8Dmaz"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# We disable tensorflow eager execution mode to enhance CPU and RAM efficiency\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A80O17VnmDkL"
   },
   "source": [
    "## 0. Introduction\n",
    "**(0 points)**\n",
    "\n",
    "This notebook is intended for demonstrating various **fundamental ideas of engineering a neural network** model. It happens that MNIST database offers a good platform to showcase these ideas. While the ideas discussed here are general enough and can be applied to a wide class of different neural network architectures, our \"vanilla\" models (models with just fully-connected layers) here do not scale up well to many other computer vision problems. For harder problems, more sophisticated neural network architectures are needed. To learn novel computer vision techniques, check out [convolutional neural network](https://datax.berkeley.edu/wp-content/uploads/2020/09/slides-m430-convolutional-neural-networks.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZmQgGVeJq3I"
   },
   "source": [
    "### 0.a Load MNIST Database\n",
    "\n",
    "In this homework, we will work on the [MNIST Database](http://yann.lecun.com/exdb/mnist/). The database contains hand-written images (digitized with 28-by-28 grey-scale pixels) of ten different classes (digits 0\\~9.) There are 60000 entries in the MNIST training dataset and 10000 entries in the MNIST test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nnd_W3tIEPxw",
    "outputId": "d734e92f-c118-4430-9ca9-214232ea106e"
   },
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"Shape of the full training images array:\", X_train_full.shape)\n",
    "print(\"Shape of the full training labels array:\", y_train_full.shape)\n",
    "print(\"Shape of the test images array:\",X_test.shape)\n",
    "print(\"Shape of the test labels array:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w85dfyjdqmAg"
   },
   "source": [
    "To reduce the computation power requirement, we will just use 1/6 of the MNIST training data (10000 samples, 1000 each class.) We then use a train_test_split to further split the subset into the training dataset (`X_train`, `y_train`, 6000 samples) and the validation dataset (`X_val`, `y_val`, 4000 samples) we will use for the following problems. Stratified sampling is used to ensure that the 10 classes are balanced in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dn5IX4u82Hhw",
    "outputId": "6c1028c3-5dff-4323-c66e-81eeffeb5cdf"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "## Stratified Sampling\n",
    "X_train_sample, y_train_sample = resample(X_train_full,y_train_full,\n",
    "                                          replace=False,n_samples=10000,\n",
    "                                          stratify=y_train_full,random_state=0)\n",
    "## Trainv-validation split \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_sample,y_train_sample,test_size=0.4,\n",
    "                                                  stratify=y_train_sample,random_state=0)\n",
    "del X_train_sample, y_train_sample\n",
    "print(\"Shape of the training images array:\", X_train.shape)\n",
    "print(\"Shape of the training labels array:\", y_train.shape)\n",
    "print(\"Shape of the validation images array:\",X_val.shape)\n",
    "print(\"Shape of the validation labels array:\",y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHL9YxSFtK7R"
   },
   "source": [
    "Let's take a look at the images in `X_train` and see if they match with the labels in `y_train`. We also want to make sure the data are shuffled, which is an important prerequisite for stochastic gradient descent (SGD) methods to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "Y1wqobRvGUlG",
    "outputId": "a829095c-cab6-4e44-b465-2294a9320019"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True,figsize=(15,4.5))\n",
    "spec_arr = gridspec.GridSpec(ncols=10, nrows=3, figure=fig)\n",
    "for ximg, ylabel, spec in zip(X_train,y_train,spec_arr):\n",
    "  ax = fig.add_subplot(spec)\n",
    "  ax.imshow(ximg)\n",
    "  ax.set_title(\"({0})\".format(ylabel))\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.b Preprocessing Data\n",
    "\n",
    "Each MNIST pixel value ranges from 0\\~255 (8-bit), but we scale the pixels uniformly to 0~1 to make weight initializations easier. This results in conversion of data type, and we store the scaled pixel as 16-bit (half-precision) float to save memory without sacrificing much accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train,dtype=np.float16) / 255\n",
    "X_val   = np.array(X_val  ,dtype=np.float16) / 255\n",
    "X_test  = np.array(X_test ,dtype=np.float16) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5Icvz5lqJGH"
   },
   "source": [
    "### 0.c Building and Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1Dh0Na2Dndi"
   },
   "source": [
    "#### 0.c.1 Weight Initializers\n",
    "Initialization of parameters (weights) is important for training a neural network. To name some the novel initialization methods\n",
    "- [Glorot initializer](http://proceedings.mlr.press/v9/glorot10a.html) is designed for neurons with sigmoid or tanh activation functions.\n",
    "- [He initializer](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html) is designed for neurons with Rectified Linear Unit (ReLu) like activation functions.\n",
    "\n",
    "> These are the two initializers ([`tf.keras.initializers.GlorotNormal`](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal) and [`tf.keras.initializers.HeNormal`](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal)) we will use throughout this homework. For both of initialization techniques, the initial weights are drawn randomly from a certain probability distribution. To ensure consistent results accross different runs, we set the seed for random number generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yRTNt2_yDlbo"
   },
   "outputs": [],
   "source": [
    "initializer_G = keras.initializers.GlorotNormal(seed=0)\n",
    "initializer_H = keras.initializers.HeNormal(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qP9YtWTHFalA"
   },
   "source": [
    "#### 0.c.2 Keras [Sequential Model](https://www.tensorflow.org/guide/keras/sequential_model)\n",
    "[`keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) makes it easy to create models layer-wise. The following code creates a model with three layers.\n",
    "\n",
    "1. First, the input images ($28\\times28$ pixels) are \"flattened\" into a $28\\times28=784$ dimensional vectors. The vector is input to the next layer.\n",
    "\n",
    "2. A \"Dense\" (aka \"fully connected\") layer connecting each element of the input vector to each of the 100 hidden units (neurons). There is a weight parameter associated with each of the $784\\times100$ connections, and a bias parameter for each of the 100 neurons. Therefore, the total number of parameters is $(784+1)\\times100$ for the layer. More formally, for each 784-dimensional input vector $x$, the layer outputs a 100-dimensional $y=f(w^T x+b)$ where\n",
    " - $w$ is a 784$\\times$100 weight matrix.\n",
    " - $b$ is a 100-dimensional bias vector.\n",
    " - $f$ is the non-linear activation function (elementwise \"[`sigmoid`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid)\" in our example).\n",
    "\n",
    "3. A second Dense layer which takes inputs from the first Dense layer and generates the 10-dimensional outputs of our model. Each of the ten neurons corresponds to each class label (0~9) that can be classified into. It follows similar mathematical formulation to 2, except that the activation function $f$ is \"[`softmax`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax)\" to give a probabilistic interpretation of our model outputs. This is due to the properties of softmax function:\n",
    " - Each element of the output vector is $\\in [0,1]$.\n",
    " - The summation of the 10 elements of the output vector gives 1.\n",
    "\n",
    "We then use the `keras.Model.summary` method to get a profile of the model. Does the number of parameters for each layer match with your expectations? \n",
    "\n",
    "> The initializers we declared above are used to initialize the parameters for the two Dense layers. Doing the same will help ensure the **reproducibility of your results** in the following pactices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grlAHjKUqPNQ",
    "outputId": "133131ad-b7af-4d4b-aca9-b5d5f0ef2247"
   },
   "outputs": [],
   "source": [
    "model_basic = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(units=100,activation=\"sigmoid\",kernel_initializer=initializer_G),\n",
    "    keras.layers.Dense(units=10,activation=\"softmax\",kernel_initializer=initializer_G)\n",
    "])\n",
    "model_basic.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVr55TfyGMPZ"
   },
   "source": [
    "We then configure the model for training through [`keras.Model.compile`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile). Two essential components shuold be specified aside from the layers.\n",
    "\n",
    "- The **loss function** quantifies the \"goodness\" of the model output. Model training is essentially a process trying to minimize the loss function by updating the parameters. For our multi-class classification problem, [`SparseCategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy) is a natural choice.\n",
    "- The **optimizer**, i.e. the algorithm used to minimze the loss. For deep neural networks, virtually all popular optimizers are based on [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD) method. Here we use a basic [`SGD`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) optimizer with learning rate set to 1.\n",
    "\n",
    "Additionally, you can specify extra **metrics** to keep track of during training besides the loss value. For example, [`Accuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy) can be a useful indicator of the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6ToV9PfaFt1j"
   },
   "outputs": [],
   "source": [
    "model_basic.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMU6XSVsGPOf"
   },
   "source": [
    "The `train_nn` function we define here will be used to fit the models throughout the notebook. It calls the [`keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) method of which some relevant arguments are explained below.\n",
    "- The SGD `batch_size` is set to 32, meaning for each SGD iteration (each time the weights get updated) the gradient corresponding to 32 samples are considered. A reasonable batch size helps speed up the optimization with parallelized computation.\n",
    "- The SGD `epochs` determines the number of times that all samples in `X_train` are traversed by SGD during training.\n",
    "\n",
    "> We set `shuffle=False` to make the training results repeatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "32c4Iiw1F06e",
    "outputId": "b427f7cc-85e9-448e-81a1-7582b88eac07"
   },
   "outputs": [],
   "source": [
    "def train_nn(model,epochs=5,verbose=0):\n",
    "  history = model.fit(X_train,y_train,validation_data=(X_val,y_val),\n",
    "                      batch_size=32,\n",
    "                      epochs=epochs,shuffle=False,verbose=verbose)\n",
    "  return history\n",
    "history_basic = train_nn(model_basic,verbose=1)\n",
    "pd.DataFrame(history_basic.history).plot(figsize=(5,5))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyEWrGhHHXfB"
   },
   "source": [
    "The \"history\" object returned by the fit method contains essential information of the training history. Here we keep a record of the example in `history_basic` which will searve as a baseline for various comparisons in the following sections.\n",
    "\n",
    "> Unless you have re-initialzed `model_basic`, **do not run the code cell above more than once**. This will overwrite `history_basic` with a different training history that starts with pretrained initial weights. Although this will not affect the grading, this will undermine the purpose of a baseline training history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIt7UwVVP2p8"
   },
   "source": [
    "#### 0.c.3 Making Predictions\n",
    "\n",
    "Similarly to scikit-learn estimators, you can call the [`keras.Model.predict`](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly#predict) method to make predictions. Recall that we used \"softmax\" for the ten output neurons, so the model will output ten numbers which can be interpretated as the probabilities (model's certainty) that the image belongs to each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "Dsd1pmylJMl-",
    "outputId": "936b8dab-1d44-4b2e-b41a-6e29451e8f7f"
   },
   "outputs": [],
   "source": [
    "prediction_basic_9 = model_basic.predict(X_test[:9])\n",
    "\n",
    "fig = plt.figure(constrained_layout=True,figsize=(12,6))\n",
    "spec_arr = gridspec.GridSpec(ncols=6, nrows=3, figure=fig)\n",
    "\n",
    "for n, (ximg, ylabel, pred) in enumerate(zip(X_test[:9],y_test[:9],prediction_basic_9)):\n",
    "  ax_img  = fig.add_subplot(spec_arr[2*n])\n",
    "  ax_img.imshow(ximg.astype(float))\n",
    "  ax_img.set_title(\"({0})\".format(ylabel))\n",
    "  ax_img.set_xticks([])\n",
    "  ax_img.set_yticks([])\n",
    "\n",
    "  ax_pred = fig.add_subplot(spec_arr[2*n+1])\n",
    "  ax_pred.set_xticks(range(10))\n",
    "  ax_pred.bar(range(10),pred,color='C0' if np.argmax(pred)==ylabel else 'C1')\n",
    "  ax_pred.set_ylim([0,1])\n",
    "  ax_pred.set_xlabel('Class')\n",
    "  ax_pred.set_ylabel('Modeled Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsyVRcy2kRnn"
   },
   "source": [
    "The code cell below classifies images in `X_test` and evaluates the prediction metric. You should see that the test accuracy is 0.9292."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AS27k1VmkBrz",
    "outputId": "fb660820-18f9-4692-8a49-2d0bb9538a84"
   },
   "outputs": [],
   "source": [
    "print ('test acc:',model_basic.evaluate(X_test,y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`keras.backend.clear_session`](https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session) frees your model from memory. Call it whenever necessary to remove unused models, especially if you work on a platform that is memory-limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRg79wnpeEH6"
   },
   "source": [
    "## 1. Training: Loss Minimization \n",
    "**(Total 20 points)**\n",
    "\n",
    "In this section, we dive a bit into the optimizers which are the workhorse of deep learning.\n",
    "\n",
    "Run the block below to define useful wrapper functions that will be used throughout the section.\n",
    "- `build_vanilla_NN` builds the neural network model we want to train. The network structure is the same as in 0.c.2. However, the `optimizer` argument remains undeclared, which is exactly what you will work on. \n",
    "- We also redefine the `train_nn` method here specifying the training and validation data, batch size, and shuffling criteria. Simply call the method with your compiled model to initiate the training procudure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Gxlqs8YJnBoG"
   },
   "outputs": [],
   "source": [
    "def build_vanillaNN(optimizer):\n",
    "  model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(100,activation=\"sigmoid\",kernel_initializer=initializer_G),\n",
    "    keras.layers.Dense(10,activation=\"softmax\",kernel_initializer=initializer_G)\n",
    "  ])\n",
    "  model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=optimizer,\n",
    "                metrics=[\"accuracy\"])\n",
    "  return model\n",
    "\n",
    "def train_nn(model,epochs=5,verbose=0,validation=True):\n",
    "  if validation:\n",
    "    history = model.fit(X_train,y_train,validation_data=(X_val,y_val),\n",
    "                        batch_size=32,epochs=epochs,shuffle=False,verbose=verbose)\n",
    "  else:\n",
    "    history = model.fit(X_train,y_train,\n",
    "                        batch_size=32,epochs=epochs,shuffle=False,verbose=verbose)\n",
    "  return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYpOTT-WX8mi"
   },
   "source": [
    "- `plot_learning` is a helper function that can be used to visualize the learning curves. It can be used to plot learning curves from different models together for the ease of comparisons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_lxmmL3jt9eM"
   },
   "outputs": [],
   "source": [
    "def plot_learning(history,label,c,ax_loss,ax_acc):\n",
    "  train_label = \"train ({0})\".format(label)\n",
    "  val_label   = \"validation ({0})\".format(label)\n",
    "  if 'loss' in history.history:\n",
    "    ax_loss.plot(history.history['loss'],c=c,label=train_label)\n",
    "  if 'val_loss' in history.history:\n",
    "    ax_loss.plot(history.history['val_loss'],'--',c=c,label=val_label)\n",
    "  if 'accuracy' in history.history:\n",
    "    ax_acc.plot(history.history['accuracy'],c=c,label=train_label)\n",
    "  if 'val_accuracy' in history.history:\n",
    "    ax_acc.plot(history.history['val_accuracy'],'--',c=c,label=val_label)\n",
    "\n",
    "  for ax in [ax_loss,ax_acc]:\n",
    "    ax.legend()\n",
    "    ax.grid('on')\n",
    "    ax.set_xlabel(\"epoch\")\n",
    "  ax_loss.set_ylabel(\"Loss\")\n",
    "  ax_acc.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k66xeaGJg--"
   },
   "source": [
    "### 1.a Learning Rate\n",
    "\n",
    "In 0.c.2, we trained the network using a plain SGD optimizer with learning rate fixed to 1, which yielded reasonable learning curve. What if we increase or decrease the learning rate? Feel free to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jp23M-dtiPmX"
   },
   "source": [
    "#### 1.a.1 Constant Learning Rate\n",
    "**(5 points)**\n",
    "\n",
    "Build a model through `build_vanillaNN`. Find a SGD learning rate that makes the average training error $\\le$ 0.1 for the 5th epoch.\n",
    "\n",
    "**Submission format:**\n",
    "- The final model (compiled with [`SGD`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) optimizer with the learning rate you find) should be stored in **`model_1a`**.\n",
    "- Run `train_nn(model_1a)` to generate the learning curve. You should be able to see the curves through the last three lines of codes.\n",
    "  - You can see from the plots whether you will pass the test, i.e. `history_1a1.history['loss'][4]<=0.1`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a1\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "la5gMQf1vyAi",
    "outputId": "76e0dfe0-db8c-4bf3-be3f-96046de44aa5"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "## Your code here\n",
    "...\n",
    "model_1a1 = ...\n",
    "\n",
    "history_1a1 = train_nn(model_1a1)\n",
    "\n",
    "## Visualizing learning curves\n",
    "lr_num = keras.backend.eval(history_1a1.model.optimizer.learning_rate)\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4))\n",
    "plot_learning(history_basic,'lr=1.0','C0',AX[0],AX[1])\n",
    "plot_learning(history_1a1,'lr={0}'.format(lr_num),'C1',AX[0],AX[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z1zZTHXc3Jj"
   },
   "source": [
    "Why is there an optimal learning rate to minimize the training error over a few epochs? What happens when the learning rate is too large/small?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SIgJNl1pTMbV"
   },
   "source": [
    "#### 1.a.2 Scheduled Learning Rate\n",
    "**(5 points)**\n",
    "\n",
    "A large learning rate may work well when the model is far from the optimum. As the training process gradually converges towards the optimum, a lower learning rate usually works better than the large learning rate that was ideal in the beginning. \n",
    "\n",
    "Therefore, an adaptive learning rate can speed up the overall training significantly. For example, we can schedule the learning rate to decrease over steps based on a certain criterion. The [keras LearningRateSchedule](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule) class provides the flexibility to specify this manually. Let's try to utilize it and beat your training result from 1.a.1.\n",
    "\n",
    "Build a model through `build_vanillaNN`. Design an SGD learning rate scheduler that enables better training performance than 1.a.1.\n",
    "\n",
    "**Submission format:**\n",
    "- When declaring an [`SGD`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) optimizer, pass a [`LearningRateSchedule`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/LearningRateSchedule) based object (e.g. [`InverseTimeDecay`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/InverseTimeDecay?version=nightly), [`ExponentialDecay`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay?version=nightly)) as the argument for `learning_rate`. You have the freedom to set the scheduling criterion.\n",
    "- The final model compiled with the SGD optimizer specified above should be stored in **`model_1a2`**.\n",
    " - We will check if the `model_1a2.optimizer.learning_rate` is a class in [`keras.optimizers.schedules`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules?version=nightly).\n",
    "- `train_nn(model_1a2)` generates the learning curves. The following plots compare your new results with the results from 1.a.1. You will pass the second test if the average training loss of the 5th epoch is lower than that from 1.a.1.\n",
    " - That is, `history_1a2.history['loss'][4] < history_1a1.history['loss'][4]`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a2\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "QWQOrb9Bw4y5",
    "outputId": "bdb3875a-c1ca-44e0-8cb0-b55dc2b8c16e"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "from tensorflow.keras.optimizers.schedules import *\n",
    "## Your code here\n",
    "...\n",
    "\n",
    "history_1a2 = train_nn(model_1a2)\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4.5))\n",
    "plot_learning(history_1a1,'1a1: fixed lr','C0',AX[0],AX[1])\n",
    "plot_learning(history_1a2,'1a2: scheduled lr','C1',AX[0],AX[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yzjkdHWFJk16"
   },
   "source": [
    "### 1.b Momentum\n",
    "**(5 points)**\n",
    "\n",
    "[Momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) is another technique that accelerates the gradient descent. It has a theoretical justification for convex optimization problems, and empirically works well for some non-convex problems such as neural network training. For `tensorflow.keras`, `momentum` is another hyperparameter that can be tuned for the [`SGD`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) optimizer. We will utilize it to speed up our training in this problem.\n",
    "\n",
    "Build a model through `build_vanillaNN`. Find an SGD momentum damping rate that makes the average training error < 0.08 for the 5th epoch.\n",
    "\n",
    "**Submission format:**\n",
    "- Declare an [`SGD`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD) optimizer with `learning_rate=1.0` (same as 0.c.2) and non-zero `momentum` term.\n",
    "- Find a `momentum` value that makes the average training loss of the 5th epoch < 0.08. The model compiled with the SGD optimizer specified above should be stored in **`model_1b`**.\n",
    " - The momentum value should be > 0.\n",
    "- Run `train_nn(model_1b)` to generate the learning curve.\n",
    " - We will test if `history_1b.history['loss'][4] < 0.08`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "z295gc9RSWZw",
    "outputId": "f65e8a26-d08f-4434-b852-2138dfc34569"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "## Your code here\n",
    "...\n",
    "model_1b = ...\n",
    "\n",
    "history_1b = train_nn(model_1b)\n",
    "\n",
    "## Visualizing learning curves\n",
    "momentum_num = keras.backend.eval(history_1b.model.optimizer.momentum)\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4))\n",
    "plot_learning(history_basic,'no momentum','C0',AX[0],AX[1])\n",
    "plot_learning(history_1b,'momentum={0:.2f}'.format(momentum_num),'C1',AX[0],AX[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "S4n798viegll"
   },
   "source": [
    "### 1.c More Optimizers\n",
    "**(5 points)**\n",
    "\n",
    "There are algorithms based on SGD but try to outsmart it by automatically adapting the update step sizes. To name a few implemented in keras,\n",
    "- [`keras.optimizers.Adagrad`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad)\n",
    "- [`keras.optimizers.RMSprop`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop)\n",
    "- [`keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n",
    "\n",
    "To learn more details about them, see the reference in the API pages. Pick one of the advanced optimizers and try it out by training the model with `build_vanillaNN`. \n",
    "\n",
    "**Submission format:**\n",
    "- Declare an optimizer that is not `SGD`, set relavent hyperparameters that can result in good training performance.\n",
    "- The model compiled with the advanced optimizer specified above should be stored in **`model_1c`**.\n",
    "- Run `train_nn(model_1c)` to generate the learning curve.\n",
    " - We will test if `history_1c.history['loss'][4] < 0.07`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "id": "EfhSLEfTz5Iv",
    "outputId": "5bdc0e8e-c3ca-4589-f347-25f13156127d"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "## Your code here\n",
    "...\n",
    "model_1c = ...\n",
    "\n",
    "history_1c = train_nn(model_1c)\n",
    "\n",
    "## Visualizing learning curves\n",
    "optimizer_name = history_1c.model.optimizer.__class__.__name__\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4))\n",
    "plot_learning(history_1a1,'1a1: SGD','C0',AX[0],AX[1])\n",
    "plot_learning(history_1c,'1c: {0}'.format(optimizer_name),'C1',AX[0],AX[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh1O7Eb1eWMX"
   },
   "source": [
    "## 2. Customizing Neural Networks\n",
    "**(Total 30 points)**\n",
    "\n",
    "In this section, we will keep working on \"shallow\" neural networks (with just one hidden layer,) but we will learn the interesting effects of hyperparameters of the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgqs7Yz1ek2B"
   },
   "source": [
    "### 2.a Activation Functions\n",
    "\n",
    "In the previous sections, we used the bio-inspired \"sigmoid\" as the activation function of the hidden layer. It possesses certain nice properties. For example, it is good at learning logical functions. Although it was a popular choice for a long time, it results in gradients that are hard to handle. In principle, any non-linear functions may be used as activation functions for a neural network. There are some other activation functions that have been proven useful:\n",
    "\n",
    "1. **tanh** is similar to sigmoid, but it allows the output to be negative. Therefore, it allows for a larger reachable parameter space than sigmoid.\n",
    "\n",
    "2. **Rectified Linear Unit (ReLu)** has constant gradients in the positive region, so it results in more controllable gradients than sigmoid. Besides, it is computationally efficient. However, it is unable to learn (due to 0 gradient) unless it outputs non-zero values for any samples. This is called the dying ReLu problem.\n",
    "\n",
    "3. **Exponential Linear Unit (ELU)** is similar to ReLu but is differentiable everywhere, making it numerically more stable than ReLu. Moreover, it allows for negative ouput which alleviates the dying problem.\n",
    "\n",
    "Note that there is no silver bullet activation function that works the best for all networks. For each problem, the activation function can be chosen through (cross-)validation. \n",
    "This motivates us to build models with the newly learned activation functions. For all questions under 2.a, you will still create models with 3 layers as specified below:\n",
    "- The first layer should be a `Flatten` layer. \n",
    "- The second layer should be a `Dense` layer with 100 neurons.\n",
    "- The third layer, the output layer, should be a `Dense` layer with 10 neurons with softmax activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "aQfmVj3tfwUx"
   },
   "source": [
    "#### 2.a.1 Hyperbolic Tangent (tanh)\n",
    "**(5 points)**\n",
    "\n",
    "Train a model with the hidden layer activation function being \"[`tanh`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh).\" Use any optimization techniques to achieve low training and validation losses.\n",
    "\n",
    "**Submission format:**\n",
    "- You have the freedom to use any initializers and optimizers, but make sure the activation function of the second layer is \"tanh\".\n",
    "- The model compiled with the designed optimizer specified above should be stored in **`model_2a1`**.\n",
    "- Run `train_nn(model_2a1)` to generate the learning curves.\n",
    " - We will test if `history_2a1.history['loss'][4] < 0.08`.\n",
    " - We will test if `history_2a1.history['val_loss'][4] < 0.28`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a1\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "q-aHh3sXfvgk",
    "outputId": "65a59394-2f94-4582-f63f-c4eb05dbfc43"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "## Your code here\n",
    "...\n",
    "\n",
    "model_2a1 = ...\n",
    "\n",
    "history_2a1 = train_nn(model_2a1,epochs=5)\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4.5))\n",
    "plot_learning(history_2a1,'2a1: tanh','C0',AX[0],AX[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9RF4SqMqfYEm"
   },
   "source": [
    "#### 2.a.2 Rectified Linear Unit (ReLu)\n",
    "**(5 points)**\n",
    "\n",
    "Train a model with the hidden layer activation function being \"[`relu`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu).\" Use any optimization techniques to achieve low training and validation losses.\n",
    "\n",
    "**Submission format:**\n",
    "- You have the freedom to use any initializers and optimizers, but make sure the activation function of the second layer is \"relu\".\n",
    "- The model compiled with the designed optimizer specified above should be stored in **`model_2a2`**.\n",
    "- Run `train_nn(model_2a2)` to generate the learning curves.\n",
    " - We will test if `history_2a2.history['loss'][4] < 0.08`.\n",
    " - We will test if `history_2a2.history['val_loss'][4] < 0.28`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a2\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "7jTPCqSJR10I",
    "outputId": "69e24fcc-51e3-4be0-b149-3c7ba7cddce0"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "## Your code here\n",
    "...\n",
    "\n",
    "model_2a2 = ...\n",
    "\n",
    "history_2a2 = train_nn(model_2a2,epochs=5)\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4.5))\n",
    "plot_learning(history_2a2,'2a2: relu','C0',AX[0],AX[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lNlfM6PyYbTh"
   },
   "source": [
    "#### 2.a.3 Exponential Linear Unit (ELU)\n",
    "**(5 points)**\n",
    "\n",
    "Train a model with the hidden layer activation function being \"[`elu`](https://www.tensorflow.org/api_docs/python/tf/keras/activations/elu).\" Use any optimization techniques to achieve low training and validation losses.\n",
    "\n",
    "**Submission format:**\n",
    "- You have the freedom to use any initializers and optimizers, but make sure the activation function of the second layer is \"elu\".\n",
    "- The model compiled with the designed optimizer specified above should be stored in **`model_2a3`**.\n",
    "- Run `train_nn(model_2a3)` to generate the learning curves.\n",
    " - We will test if `history_2a3.history['loss'][4] < 0.10`.\n",
    " - We will test if `history_2a3.history['val_loss'][4] < 0.28`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a3\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "g08gKh06Ybs_",
    "outputId": "eeb2d10a-d5b6-42be-fbf9-2c63f7fb1e5e"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "## Your code here\n",
    "...\n",
    "\n",
    "model_2a3 = ...\n",
    "\n",
    "history_2a3 = train_nn(model_2a3,epochs=5)\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4.5))\n",
    "plot_learning(history_2a3,'2a3: elu','C0',AX[0],AX[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS7ktVqreqR7"
   },
   "source": [
    "### 2.b Number of Neurons\n",
    "**(15 points)**\n",
    "\n",
    "In our model, the number of parameters increases linearly with the \"width\" of the hidden layer. How does that affect the training and validation performance?\n",
    "\n",
    "Create models with 3 layers, but vary the number of neurons from 50 to 200 for the hidden layer:\n",
    "- The first layer should be a `Flatten` layer. \n",
    "- The second layer should be a `Dense` layer with `sigmoid` activations and `kernel_initializer=initializer_G`. `units` should be set according to the elements in `nunits_grid`.\n",
    "- The third layer, the output layer, should be a `Dense` layer with 10 neurons with softmax activation.\n",
    "\n",
    "**Submission format:**\n",
    "- For each element in `nunits_grid` in ascending order, build and train (you can still use `train_nn`) a model according to above specifications. Use **the same optimizer** to train all the models.\n",
    "- Append the training history for each model to the list `histories_2b`. That is, we should have `histories_2b[0]` being results for `units=50`, `histories_2b[1]` being results for `units=100`, and so on.\n",
    "  - For the model with `units=50`, you shuold have average training loss for the 5th epoch < 0.1.\n",
    "  - We expect the training loss not to increase as the `nunits` increases. This will be tested for `units` in `[100,150]` through `histories_2b[1:3]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "zpHy5Rkh9sQC",
    "outputId": "110519c8-8a35-4675-b29b-76954381976c"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "nunits_grid  = [50,100,150]\n",
    "histories_2b = [] \n",
    "# Your code here\n",
    "...\n",
    "\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4.5))\n",
    "Colors = ['C0','C1','C2']\n",
    "for history, nunits, c in zip(histories_2b,nunits_grid,Colors):\n",
    "  plot_learning(history,'{0}'.format(nunits),c,AX[0],AX[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b1\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "k_U5bGsH2Omv"
   },
   "outputs": [],
   "source": [
    "# tests for units=50 (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "nFdcL7BUgfLp"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b2\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "omhJNXzLJPv4"
   },
   "outputs": [],
   "source": [
    "# tests for units=100 (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fdyTKLY1ghZB"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b3\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "dpAQMVjwJx4q"
   },
   "outputs": [],
   "source": [
    "# tests for units=150 (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtldKbciKWgy"
   },
   "source": [
    "How did the validation loss and accuracy change as you increased `units`? You may find that with \\~120K parameters for 150 hidden neurons, the overfitting problem is not worse than the case with 100 neurons (\\~80K parameters). More interestingly, there is evidence showing that [over-parameterization helps neural network generalize](https://openreview.net/forum?id=BygfghAcYX)! This is a curious behaviour of neural network that was not expected in conventional machine learning theory, and is one of the reasons that gigantic neural networks are used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br-kgFzucUSU"
   },
   "source": [
    "## 3. Deep Neural Networks \n",
    "**(Total 20 points)**\n",
    "\n",
    "A \"deep\" neural network has more than one hidden layers. This is motivated by the observation that a deep nerwork is able to learn features at different levels of abstractions. The deeper the data get passed down the network, the higher the abstraction level. This allows archirectures such as convolutional neural network (CNN) to learn well-structured and useful feature maps. Therefore, practical CNN usually has tens or hundreds of layers. For plain fully-connected architecture such as the one we have here, networks with 2 hidden layers usually outperform networks with 1 hidden layer. However, stacking even more fully-connected layers may not further boost performance of fully-connected networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EwdBagUTFU-"
   },
   "source": [
    "### 3.a More Hidden Layers\n",
    "**(8 points)**\n",
    "\n",
    "Create and train models with more hidden layers! Here we will build models with up to 3 hidden layers.\n",
    "\n",
    "**Submission format:**\n",
    "- For each element in `nhlayers_grid` in ascending order, build and train (you can still use `train_nn`) a model with the corresponding number of hidden `Dense` layers. Use **the same optimizer** to train all the models.\n",
    "- For each model, the first layer should still be a `Flatten` layer, and the final layer should still be a `Dense` layer with 10 units and softmax activation. Therefore, the **total number of layers should be 2+(# of hidden layers)**.\n",
    "- Append the training history for each model to the list `histories_3a`. That is, we should have `histories_3a[0]` being results for `nhlayers=1`, `histories_3a[1]` being results for `nhlayers=2`, and so on.\n",
    "- There is no training loss requirement for this question. We will simply test if your models comply to the specifications above. \n",
    "- We suggest the following hyperparameters:\n",
    " - Use default `RMSprop` optimizer.\n",
    " - For each hidden layer, `units=100`, `activation='relu'`, and `kernel_initializer=initializer_H`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "ChFlfQM8Qclk",
    "outputId": "e8c8ffed-e3f9-4a9e-e0e0-5345a3d64f9f"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "nhlayers_grid = [1,2,3]\n",
    "histories_3a  = []\n",
    "# Your code here\n",
    "...\n",
    "\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4.5))\n",
    "for history,n,c in zip(histories_3a,nhlayers_grid,['C0','C1','C2','C3']):\n",
    "  plot_learning(history,'{0}'.format(n), c,AX[0],AX[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a1\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "BgHTMBNv10bN"
   },
   "outputs": [],
   "source": [
    "# test for 2 hidden dense (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9GlnYoiog0FM"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a2\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "Cxa5edvsvlmo"
   },
   "outputs": [],
   "source": [
    "# test for 3 hidden dense (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxLak0_zFOwj"
   },
   "source": [
    "### 3.b Batch Normalization\n",
    "**(12 points)**\n",
    "\n",
    "As the network gets deeper, it becomes harder to train the network because the gradients are easier to either explode or vanish. An effective way to counter this problem is through [`BatchNormalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=nightly) which analogously would attempt to standardize the inputs for each layer. It is also a great way to accelerate the training. Now, we are going to add it to our model and see how much it helps.\n",
    "\n",
    "**Submission format:**\n",
    "- For each element in `nhDenselayers_grid` in ascending order, repeat what you did in the last question, but now add a `BatchNormalization` layer after each `Dense` hidden layer.\n",
    "- For each model, the first layer should still be a `Flatten` layer, and the final layer should still be a `Dense` layer with 10 units and softmax activation. The number of hidden layers get doubled from before because of the newly-added `BatchNormalization` layer. Therefore, the **total number of layers should be 2+2$\\times$(# of hidden Dense layers)**.\n",
    "- Append the training history for each model to the list `histories_3b`. That is, we should have `histories_3b[0]` being results for `nhDenselayers=1`, `histories_3b[1]` being results for `nhDenselayers=2`, and so on.\n",
    "- The other suggestions follow 3.a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "o2DGT3zcVLFY",
    "outputId": "784b9b2a-dcd4-4abd-85c0-4f5bf5b53204"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "\n",
    "nhDenselayers_grid = [1,2,3]\n",
    "histories_3b  = []\n",
    "# Your code here\n",
    "...\n",
    "\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4.5))\n",
    "for history,n,c in zip(histories_3b,nhDenselayers_grid,['C0','C1','C2','C3']):\n",
    "  plot_learning(history,'{0}'.format(n), c,AX[0],AX[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b1\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "If9kKhJE1x-8"
   },
   "outputs": [],
   "source": [
    "# test for 1 hidden dense (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_yrWV98Mg6vc"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b2\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "XYBwg1IovjJy"
   },
   "outputs": [],
   "source": [
    "# test for 2 hidden dense (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kEHi4e-0g9DM"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b3\n",
    "manual: false\n",
    "points: 4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "3p6IGZRRve_S"
   },
   "outputs": [],
   "source": [
    "# test for 3 hidden dense (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mE9jMZmfcXP"
   },
   "source": [
    "You should be able to see substantial improvements in the training loss. Note that inserting Batch Normalization layer after the linear layer and right before the activation function is used more commonly than what we have done here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKxJ-PTLeJbb"
   },
   "source": [
    "## 4. Regularizations\n",
    "**(Total 20 points)**\n",
    "\n",
    "Being heavily parameterized nonlinear models, neural networks are prone to overfitting. Overfitting may be resolved either by including more training data or through regularization (e.g. penalizing large weights as we have learned in HW2). Aside from weight penalies (see [`tf.keras.regularizers`](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)), there exists an interesting and powerful regularization technique for neural network - [Dropout](https://cs231n.github.io/neural-networks-2/#reg). It is also implemented in tensorflow keras as [`tf.keras.layers.Dropout`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout).\n",
    "\n",
    "Now, apply any regularization technique to reduce the validation loss for a neural network with the following specifications:\n",
    "- There should by only one hidden Dense layer with 100 units and `relu` activation.\n",
    "- You may add any numbers of `Dropout` layers, but do not add layers with parameters (such as `BatchNorm`). `Dropout` layers do not have parameters. The dropout rate is a hyperparameter.\n",
    "- Therefore, the number of parameters is fixed to 79510. This will be tested to ensure **everyone has the same baseline model**.\n",
    "- You are free to use **any initializer, optimizers and regularizers with the hyperparameters you like**.\n",
    "\n",
    "**Submission format:**\n",
    "- The model compiled with the optimizer and regularizersshould be stored in **`model_4`**.\n",
    "- Run `train_nn(model_4)` to generate the learning curves. Your grade will be determined by the validation loss after 10 epochs of training.\n",
    " - You get 100% marks if `history_4.history['val_loss'][9] < 0.20`.\n",
    " - You get 75% marks if `0.20 <= history_4.history['val_loss'][9] < 0.21`.\n",
    " - You get 50% marks if `0.21 <= history_4.history['val_loss'][9] < 0.22`.\n",
    " - You get 25% marks if `0.22 <= history_4.history['val_loss'][9] < 0.23`.\n",
    "\n",
    " **Important:** To make sure your validation loss is reproducible by the grader, **seed everything you use that requires random number generations**. This may include the `kernel_initializer` and the `Dropout` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "vKAo2BOZHepA"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "## Your code here\n",
    "...\n",
    "\n",
    "model_4 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "WSByF-_wuMwE",
    "outputId": "77b3b034-984a-49fd-d89e-2eb7948cfe1a"
   },
   "outputs": [],
   "source": [
    "history_4 = train_nn(model_4,epochs=10)\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4.5))\n",
    "plot_learning(history_4,'Prob 4','C0',AX[0],AX[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q41\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "uJRIyuWJ1ryq"
   },
   "outputs": [],
   "source": [
    "# test for val loss < 0.20 (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q41\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uqaU9wM7hPy-"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q42\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "V41sOAZzuuSu"
   },
   "outputs": [],
   "source": [
    "# test for val loss < 0.21 (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HeAJGdoXhTNS"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q43\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "skufhNI5uyEq"
   },
   "outputs": [],
   "source": [
    "# test for val loss < 0.22 (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q43\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "76yWuP2vhVVR"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q44\n",
    "manual: false\n",
    "points: 5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "OJbRnf8LuzIe"
   },
   "outputs": [],
   "source": [
    "# test for val loss < 0.23 (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q44\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning\n",
    "**(Total 30 points)**\n",
    "\n",
    "Equipped with all the techniques we just learned, you are ready to build your own neural network!\n",
    "- For this question, we unlock the full training dataset. Your model will be trained on 80% of `(X_train_full,y_train_full)` for 5 epochs and be validated on the rest 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not execute this cell more than once, or you need to reload MNIST\n",
    "X_train_full = np.array(X_train_full,dtype=np.float16)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7wWsaIJFSoj"
   },
   "source": [
    "- There are no restrictions on the model structure, but make sure your training can be **finished in a reasonable time span** (say, < 2 minutes.) \n",
    "- **\"accuracy\"** should be added to the tracked metrics when you compile the model.\n",
    "- Store your final model in **`model_final`**. Your model's validation accuracy after 5 epochs of training will determine the points you get. The code cell following the problem cell handles the training and validation.\n",
    " - You get 100% marks if the validation score is > 97.00%.\n",
    " - You get 75% marks if the validation score is > 96.75% but < 97.00%.\n",
    " - You get 50% marks if the validation score is > 96.50% but < 96.75%.\n",
    " - You get 25% marks if the validation score is > 96.25% but < 96.50%\n",
    "\n",
    " **Important:** To make sure your validation loss is reproducible by the grader, **seed everything you use that requires random number generations**. This may include the `kernel_initializer` and the `Dropout` layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "kD6U57lKyIVC"
   },
   "outputs": [],
   "source": [
    "clear_session()\n",
    "## Your code here\n",
    "...\n",
    "model_final = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "id": "8mzr_G71U-BL",
    "outputId": "9f36cf84-d31a-4831-ef65-854776d643e8"
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer\n",
    "\n",
    "assert (model_final.history is None),('Do not pass a model that has been trained before')\n",
    "start_time = default_timer()\n",
    "history = model_final.fit(X_train_full,y_train_full,validation_split=0.2,\n",
    "                          epochs=5,batch_size=32,shuffle=False,verbose=1)\n",
    "end_time = default_timer()\n",
    "print ('Elapsed training time: {0}s'.format(end_time-start_time))\n",
    "fig, AX = plt.subplots(nrows=1,ncols=2,figsize=(9,4.5))\n",
    "plot_learning(history,'{0}'.format(''), 'C1',AX[0],AX[1])\n",
    "print ('validation accuracy:', history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q51\n",
    "manual: false\n",
    "points: 7.5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "sG99apwkX3J8"
   },
   "outputs": [],
   "source": [
    "# test for accuracy > 0.9700 (7.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q51\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ZZLOfLMzhkB7"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q52\n",
    "manual: false\n",
    "points: 7.5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "hJP1HzLoX_VM"
   },
   "outputs": [],
   "source": [
    "# test for accuracy > 0.9675 (7.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q52\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "laqgYdZ1hl8U"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q53\n",
    "manual: false\n",
    "points: 7.5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "jFs5LrTkYArE"
   },
   "outputs": [],
   "source": [
    "# test for accuracy > 0.9650 (7.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q53\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_cfHBiAvhoR9"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q54\n",
    "manual: false\n",
    "points: 7.5\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "hA_Swj_RYBvc"
   },
   "outputs": [],
   "source": [
    "# test for accuracy > 0.9625 (7.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q54\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm5cQVC5VdFI"
   },
   "source": [
    "Finally, we have the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gy-LMXZRVR2U",
    "outputId": "343c2f3a-0eae-4a65-d319-cc853f88bad8"
   },
   "outputs": [],
   "source": [
    "model_final.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to create a pdf for your reference."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_NeuralNet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
